{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence : Although these ideas may look true in many instances, they don’t really define what the idea behind a paragraph is. This is one of those subtle things in English writing that never really gets explained on priority making it one of those commonly used things that are barely understood. Which is why this read is going to be great.\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Enter a sentence : \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Although these ideas may look true in many instances, they don’t really define what the idea behind a paragraph is.',\n",
       " 'This is one of those subtle things in English writing that never really gets explained on priority making it one of those commonly used things that are barely understood.',\n",
       " 'Which is why this read is going to be great.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = word_tokenize(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stopwords = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = []\n",
    "for each_word in tokenized_words:\n",
    "    if not each_word in list_of_stopwords:\n",
    "        clean_words.append(each_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 39)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_words), len(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Although', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('ideas', 'NNS'),\n",
       " ('may', 'MD'),\n",
       " ('look', 'VB'),\n",
       " ('true', 'JJ'),\n",
       " ('in', 'IN'),\n",
       " ('many', 'JJ'),\n",
       " ('instances', 'NNS'),\n",
       " (',', ','),\n",
       " ('they', 'PRP'),\n",
       " ('don', 'VBP'),\n",
       " ('’', 'JJ'),\n",
       " ('t', 'NN'),\n",
       " ('really', 'RB'),\n",
       " ('define', 'VB'),\n",
       " ('what', 'WP'),\n",
       " ('the', 'DT'),\n",
       " ('idea', 'NN'),\n",
       " ('behind', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('paragraph', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('subtle', 'JJ'),\n",
       " ('things', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('English', 'NNP'),\n",
       " ('writing', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('never', 'RB'),\n",
       " ('really', 'RB'),\n",
       " ('gets', 'VBZ'),\n",
       " ('explained', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('priority', 'NN'),\n",
       " ('making', 'VBG'),\n",
       " ('it', 'PRP'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('commonly', 'NNS'),\n",
       " ('used', 'VBD'),\n",
       " ('things', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('are', 'VBP'),\n",
       " ('barely', 'RB'),\n",
       " ('understood', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Which', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('why', 'WRB'),\n",
       " ('this', 'DT'),\n",
       " ('read', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('great', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'choke'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Choking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'singer'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Singer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'end'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'enabl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doctor'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'machin'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'facebook'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although --> although\n",
      "these --> these\n",
      "ideas --> idea\n",
      "may --> may\n",
      "look --> look\n",
      "true --> true\n",
      "in --> in\n",
      "many --> mani\n",
      "instances --> instanc\n",
      ", --> ,\n",
      "they --> they\n",
      "don --> don\n",
      "’ --> ’\n",
      "t --> t\n",
      "really --> realli\n",
      "define --> defin\n",
      "what --> what\n",
      "the --> the\n",
      "idea --> idea\n",
      "behind --> behind\n",
      "a --> a\n",
      "paragraph --> paragraph\n",
      "is --> is\n",
      ". --> .\n",
      "This --> thi\n",
      "is --> is\n",
      "one --> one\n",
      "of --> of\n",
      "those --> those\n",
      "subtle --> subtl\n",
      "things --> thing\n",
      "in --> in\n",
      "English --> english\n",
      "writing --> write\n",
      "that --> that\n",
      "never --> never\n",
      "really --> realli\n",
      "gets --> get\n",
      "explained --> explain\n",
      "on --> on\n",
      "priority --> prioriti\n",
      "making --> make\n",
      "it --> it\n",
      "one --> one\n",
      "of --> of\n",
      "those --> those\n",
      "commonly --> commonli\n",
      "used --> use\n",
      "things --> thing\n",
      "that --> that\n",
      "are --> are\n",
      "barely --> bare\n",
      "understood --> understood\n",
      ". --> .\n",
      "Which --> which\n",
      "is --> is\n",
      "why --> whi\n",
      "this --> thi\n",
      "read --> read\n",
      "is --> is\n",
      "going --> go\n",
      "to --> to\n",
      "be --> be\n",
      "great --> great\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "for each_word in tokenized_words:\n",
    "    print(each_word,\"-->\",ps.stem(each_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('Machine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Friend'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('Friend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employing'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('Employing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Smoking'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('Smoking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'demonetization'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('demonetization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'travelled'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('travelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.lemmatize('Google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Although --> Although\n",
      "these --> these\n",
      "ideas --> idea\n",
      "may --> may\n",
      "look --> look\n",
      "true --> true\n",
      "in --> in\n",
      "many --> many\n",
      "instances --> instance\n",
      ", --> ,\n",
      "they --> they\n",
      "don --> don\n",
      "’ --> ’\n",
      "t --> t\n",
      "really --> really\n",
      "define --> define\n",
      "what --> what\n",
      "the --> the\n",
      "idea --> idea\n",
      "behind --> behind\n",
      "a --> a\n",
      "paragraph --> paragraph\n",
      "is --> is\n",
      ". --> .\n",
      "This --> This\n",
      "is --> is\n",
      "one --> one\n",
      "of --> of\n",
      "those --> those\n",
      "subtle --> subtle\n",
      "things --> thing\n",
      "in --> in\n",
      "English --> English\n",
      "writing --> writing\n",
      "that --> that\n",
      "never --> never\n",
      "really --> really\n",
      "gets --> get\n",
      "explained --> explained\n",
      "on --> on\n",
      "priority --> priority\n",
      "making --> making\n",
      "it --> it\n",
      "one --> one\n",
      "of --> of\n",
      "those --> those\n",
      "commonly --> commonly\n",
      "used --> used\n",
      "things --> thing\n",
      "that --> that\n",
      "are --> are\n",
      "barely --> barely\n",
      "understood --> understood\n",
      ". --> .\n",
      "Which --> Which\n",
      "is --> is\n",
      "why --> why\n",
      "this --> this\n",
      "read --> read\n",
      "is --> is\n",
      "going --> going\n",
      "to --> to\n",
      "be --> be\n",
      "great --> great\n",
      ". --> .\n"
     ]
    }
   ],
   "source": [
    "for each_word in tokenized_words:\n",
    "    print(each_word,\"-->\",lm.lemmatize(each_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0      570306133677760513           neutral                        1.0000   \n",
       "1      570301130888122368          positive                        0.3486   \n",
       "2      570301083672813571           neutral                        0.6837   \n",
       "3      570301031407624196          negative                        1.0000   \n",
       "4      570300817074462722          negative                        1.0000   \n",
       "...                   ...               ...                           ...   \n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence         airline  \\\n",
       "0                         NaN                        NaN  Virgin America   \n",
       "1                         NaN                     0.0000  Virgin America   \n",
       "2                         NaN                        NaN  Virgin America   \n",
       "3                  Bad Flight                     0.7033  Virgin America   \n",
       "4                  Can't Tell                     1.0000  Virgin America   \n",
       "...                       ...                        ...             ...   \n",
       "14635                     NaN                     0.0000        American   \n",
       "14636  Customer Service Issue                     1.0000        American   \n",
       "14637                     NaN                        NaN        American   \n",
       "14638  Customer Service Issue                     0.6659        American   \n",
       "14639                     NaN                     0.0000        American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "0                        NaN          cairdin                 NaN   \n",
       "1                        NaN         jnardino                 NaN   \n",
       "2                        NaN       yvonnalynn                 NaN   \n",
       "3                        NaN         jnardino                 NaN   \n",
       "4                        NaN         jnardino                 NaN   \n",
       "...                      ...              ...                 ...   \n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "0                  0                @VirginAmerica What @dhepburn said.   \n",
       "1                  0  @VirginAmerica plus you've added commercials t...   \n",
       "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
       "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
       "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
       "...              ...                                                ...   \n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "...           ...                        ...            ...   \n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "0      Eastern Time (US & Canada)  \n",
       "1      Pacific Time (US & Canada)  \n",
       "2      Central Time (US & Canada)  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4      Pacific Time (US & Canada)  \n",
       "...                           ...  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  \n",
       "\n",
       "[14640 rows x 15 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline_names = ['virginamerica', 'southwestair', 'united', 'jetblue', 'usairways', 'americanair']\n",
    "signs = ['@', '#', '$', '%', '^', '&', '*', '(', ')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets = []\n",
    "for i in range(df.shape[0]):\n",
    "    current_tweet = df['text'].values[i]\n",
    "    current_tweet_list = word_tokenize(current_tweet)\n",
    "    clean_text = ''\n",
    "    for each_word in current_tweet_list:\n",
    "        each_word = each_word.lower()\n",
    "        if not each_word in list_of_stopwords:\n",
    "            if not each_word in airline_names:\n",
    "                if not each_word in signs:\n",
    "                    clean_text = clean_text + each_word + \" \"\n",
    "    clean_tweets.append(clean_text)\n",
    "len(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweets'] = clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>dhepburn said .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>plus 've added commercials experience ... tack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>n't today ... must mean need take another trip !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>'s really aggressive blast obnoxious `` entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>'s really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "      <td>seriously would pay 30 flight seats n't playin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>yes , nearly every time fly vx “ ear worm ” ’ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "      <td>really missed prime opportunity men without ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "      <td>well , didn't…but ! : -d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "      <td>amazing , arrived hour early . 're good .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@VirginAmerica did you know that suicide is th...</td>\n",
       "      <td>know suicide second leading cause death among ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n",
       "      <td>lt ; 3 pretty graphics . much better minimal i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@VirginAmerica This is such a great deal! Alre...</td>\n",
       "      <td>great deal ! already thinking 2nd trip austral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@VirginAmerica @virginmedia I'm flying your #f...</td>\n",
       "      <td>virginmedia 'm flying fabulous seductive skies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@VirginAmerica Thanks!</td>\n",
       "      <td>thanks !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>sfo-pdx schedule still mia .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@VirginAmerica So excited for my first cross c...</td>\n",
       "      <td>excited first cross country flight lax mco 've...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n",
       "      <td>flew nyc sfo last week could n't fully sit sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n",
       "      <td>❤️ flying . ☺️👍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@VirginAmerica you know what would be amazingl...</td>\n",
       "      <td>know would amazingly awesome ? bos-fll please ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@VirginAmerica why are your first fares in May...</td>\n",
       "      <td>first fares may three times carriers seats ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@VirginAmerica I love this graphic. http://t.c...</td>\n",
       "      <td>love graphic . http : //t.co/ut5grrwaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>@VirginAmerica I love the hipster innovation. ...</td>\n",
       "      <td>love hipster innovation . feel good brand .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@VirginAmerica will you be making BOS&amp;gt;LAS n...</td>\n",
       "      <td>making bos gt ; las non stop permanently anyti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>@VirginAmerica you guys messed up my seating.....</td>\n",
       "      <td>guys messed seating.. reserved seating friends...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>@VirginAmerica status match program.  I applie...</td>\n",
       "      <td>status match program . applied 's three weeks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@VirginAmerica What happened 2 ur vegan food o...</td>\n",
       "      <td>happened 2 ur vegan food options ? ! least say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@VirginAmerica do you miss me? Don't worry we'...</td>\n",
       "      <td>miss ? n't worry 'll together soon .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@VirginAmerica amazing to me that we can't get...</td>\n",
       "      <td>amazing ca n't get cold air vents . vx358 noai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@VirginAmerica LAX to EWR - Middle seat on a r...</td>\n",
       "      <td>lax ewr - middle seat red eye . noob maneuver ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>@VirginAmerica hi! I just bked a cool birthday...</td>\n",
       "      <td>hi ! bked cool birthday trip , ca n't add elev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>@VirginAmerica Are the hours of operation for ...</td>\n",
       "      <td>hours operation club sfo posted online current ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>@VirginAmerica help, left expensive headphones...</td>\n",
       "      <td>help , left expensive headphones flight 89 iad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>@VirginAmerica awaiting my return phone call, ...</td>\n",
       "      <td>awaiting return phone call , would prefer use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>@VirginAmerica this is great news!  America co...</td>\n",
       "      <td>great news ! america could start flights hawai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nice RT @VirginAmerica: Vibe with the moodligh...</td>\n",
       "      <td>nice rt : vibe moodlight takeoff touchdown . m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>@VirginAmerica Moodlighting is the only way to...</td>\n",
       "      <td>moodlighting way fly ! best experience ever ! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>@VirginAmerica @freddieawards Done and done! B...</td>\n",
       "      <td>freddieawards done done ! best airline around ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>@VirginAmerica when can I book my flight to Ha...</td>\n",
       "      <td>book flight hawaii ? ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>@VirginAmerica Your chat support is not workin...</td>\n",
       "      <td>chat support working site : http : //t.co/vhp2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>@VirginAmerica View of downtown Los Angeles, t...</td>\n",
       "      <td>view downtown los angeles , hollywood sign , b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>@VirginAmerica Hey, first time flyer next week...</td>\n",
       "      <td>hey , first time flyer next week - excited ! '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>@VirginAmerica plz help me win my bid upgrade ...</td>\n",
       "      <td>plz help win bid upgrade flight 2/27 lax -- - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>@VirginAmerica I have an unused ticket but mov...</td>\n",
       "      <td>unused ticket moved new city n't fly . fly exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>@VirginAmerica are flights leaving Dallas for ...</td>\n",
       "      <td>flights leaving dallas seattle time feb 24 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>@VirginAmerica I'm #elevategold for a good rea...</td>\n",
       "      <td>'m elevategold good reason : rock ! !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>@VirginAmerica  DREAM http://t.co/oA2dRfAoQ2 h...</td>\n",
       "      <td>dream http : //t.co/oa2drfaoq2 http : //t.co/l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>@VirginAmerica wow this just blew my mind</td>\n",
       "      <td>wow blew mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Afte...</td>\n",
       "      <td>ladygaga carrieunderwood last night tribute so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood All ...</td>\n",
       "      <td>ladygaga carrieunderwood entertaining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>@VirginAmerica Is flight 769 on it's way? Was ...</td>\n",
       "      <td>flight 769 's way ? supposed take 30 minutes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>ladygaga carrieunderwood julie andrews way tho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>@VirginAmerica wish you flew out of Atlanta......</td>\n",
       "      <td>wish flew atlanta ... soon ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood Juli...</td>\n",
       "      <td>ladygaga carrieunderwood julie andrews . hands .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>@VirginAmerica Will flights be leaving Dallas ...</td>\n",
       "      <td>flights leaving dallas la february 24th ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@VirginAmerica hi! i'm so excited about your $...</td>\n",
       "      <td>hi ! 'm excited 99 lga- gt ; dal deal- 've try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>@VirginAmerica you know it. Need it on my spot...</td>\n",
       "      <td>know . need spotify stat guiltypleasures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood  I'm...</td>\n",
       "      <td>ladygaga carrieunderwood 'm lady gaga ! ! ! am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@VirginAmerica @ladygaga @carrieunderwood - Ca...</td>\n",
       "      <td>ladygaga carrieunderwood - carrie !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>@VirginAmerica New marketing song? https://t.c...</td>\n",
       "      <td>new marketing song ? https : //t.co/f2lfulcbq7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0                 @VirginAmerica What @dhepburn said.   \n",
       "1   @VirginAmerica plus you've added commercials t...   \n",
       "2   @VirginAmerica I didn't today... Must mean I n...   \n",
       "3   @VirginAmerica it's really aggressive to blast...   \n",
       "4   @VirginAmerica and it's a really big bad thing...   \n",
       "5   @VirginAmerica seriously would pay $30 a fligh...   \n",
       "6   @VirginAmerica yes, nearly every time I fly VX...   \n",
       "7   @VirginAmerica Really missed a prime opportuni...   \n",
       "8     @virginamerica Well, I didn't…but NOW I DO! :-D   \n",
       "9   @VirginAmerica it was amazing, and arrived an ...   \n",
       "10  @VirginAmerica did you know that suicide is th...   \n",
       "11  @VirginAmerica I &lt;3 pretty graphics. so muc...   \n",
       "12  @VirginAmerica This is such a great deal! Alre...   \n",
       "13  @VirginAmerica @virginmedia I'm flying your #f...   \n",
       "14                             @VirginAmerica Thanks!   \n",
       "15      @VirginAmerica SFO-PDX schedule is still MIA.   \n",
       "16  @VirginAmerica So excited for my first cross c...   \n",
       "17  @VirginAmerica  I flew from NYC to SFO last we...   \n",
       "18                    I ❤️ flying @VirginAmerica. ☺️👍   \n",
       "19  @VirginAmerica you know what would be amazingl...   \n",
       "20  @VirginAmerica why are your first fares in May...   \n",
       "21  @VirginAmerica I love this graphic. http://t.c...   \n",
       "22  @VirginAmerica I love the hipster innovation. ...   \n",
       "23  @VirginAmerica will you be making BOS&gt;LAS n...   \n",
       "24  @VirginAmerica you guys messed up my seating.....   \n",
       "25  @VirginAmerica status match program.  I applie...   \n",
       "26  @VirginAmerica What happened 2 ur vegan food o...   \n",
       "27  @VirginAmerica do you miss me? Don't worry we'...   \n",
       "28  @VirginAmerica amazing to me that we can't get...   \n",
       "29  @VirginAmerica LAX to EWR - Middle seat on a r...   \n",
       "30  @VirginAmerica hi! I just bked a cool birthday...   \n",
       "31  @VirginAmerica Are the hours of operation for ...   \n",
       "32  @VirginAmerica help, left expensive headphones...   \n",
       "33  @VirginAmerica awaiting my return phone call, ...   \n",
       "34  @VirginAmerica this is great news!  America co...   \n",
       "35  Nice RT @VirginAmerica: Vibe with the moodligh...   \n",
       "36  @VirginAmerica Moodlighting is the only way to...   \n",
       "37  @VirginAmerica @freddieawards Done and done! B...   \n",
       "38  @VirginAmerica when can I book my flight to Ha...   \n",
       "39  @VirginAmerica Your chat support is not workin...   \n",
       "40  @VirginAmerica View of downtown Los Angeles, t...   \n",
       "41  @VirginAmerica Hey, first time flyer next week...   \n",
       "42  @VirginAmerica plz help me win my bid upgrade ...   \n",
       "43  @VirginAmerica I have an unused ticket but mov...   \n",
       "44  @VirginAmerica are flights leaving Dallas for ...   \n",
       "45  @VirginAmerica I'm #elevategold for a good rea...   \n",
       "46  @VirginAmerica  DREAM http://t.co/oA2dRfAoQ2 h...   \n",
       "47          @VirginAmerica wow this just blew my mind   \n",
       "48  @VirginAmerica @ladygaga @carrieunderwood Afte...   \n",
       "49  @VirginAmerica @ladygaga @carrieunderwood All ...   \n",
       "50  @VirginAmerica Is flight 769 on it's way? Was ...   \n",
       "51  @VirginAmerica @ladygaga @carrieunderwood Juli...   \n",
       "52  @VirginAmerica wish you flew out of Atlanta......   \n",
       "53  @VirginAmerica @ladygaga @carrieunderwood Juli...   \n",
       "54  @VirginAmerica Will flights be leaving Dallas ...   \n",
       "55  @VirginAmerica hi! i'm so excited about your $...   \n",
       "56  @VirginAmerica you know it. Need it on my spot...   \n",
       "57  @VirginAmerica @ladygaga @carrieunderwood  I'm...   \n",
       "58  @VirginAmerica @ladygaga @carrieunderwood - Ca...   \n",
       "59  @VirginAmerica New marketing song? https://t.c...   \n",
       "\n",
       "                                         clean_tweets  \n",
       "0                                    dhepburn said .   \n",
       "1   plus 've added commercials experience ... tack...  \n",
       "2   n't today ... must mean need take another trip !   \n",
       "3   's really aggressive blast obnoxious `` entert...  \n",
       "4                            's really big bad thing   \n",
       "5   seriously would pay 30 flight seats n't playin...  \n",
       "6   yes , nearly every time fly vx “ ear worm ” ’ ...  \n",
       "7   really missed prime opportunity men without ha...  \n",
       "8                           well , didn't…but ! : -d   \n",
       "9          amazing , arrived hour early . 're good .   \n",
       "10  know suicide second leading cause death among ...  \n",
       "11  lt ; 3 pretty graphics . much better minimal i...  \n",
       "12  great deal ! already thinking 2nd trip austral...  \n",
       "13  virginmedia 'm flying fabulous seductive skies...  \n",
       "14                                          thanks !   \n",
       "15                      sfo-pdx schedule still mia .   \n",
       "16  excited first cross country flight lax mco 've...  \n",
       "17  flew nyc sfo last week could n't fully sit sea...  \n",
       "18                                   ❤️ flying . ☺️👍   \n",
       "19  know would amazingly awesome ? bos-fll please ...  \n",
       "20  first fares may three times carriers seats ava...  \n",
       "21           love graphic . http : //t.co/ut5grrwaaa   \n",
       "22       love hipster innovation . feel good brand .   \n",
       "23  making bos gt ; las non stop permanently anyti...  \n",
       "24  guys messed seating.. reserved seating friends...  \n",
       "25  status match program . applied 's three weeks ...  \n",
       "26  happened 2 ur vegan food options ? ! least say...  \n",
       "27              miss ? n't worry 'll together soon .   \n",
       "28  amazing ca n't get cold air vents . vx358 noai...  \n",
       "29  lax ewr - middle seat red eye . noob maneuver ...  \n",
       "30  hi ! bked cool birthday trip , ca n't add elev...  \n",
       "31  hours operation club sfo posted online current ?   \n",
       "32  help , left expensive headphones flight 89 iad...  \n",
       "33  awaiting return phone call , would prefer use ...  \n",
       "34  great news ! america could start flights hawai...  \n",
       "35  nice rt : vibe moodlight takeoff touchdown . m...  \n",
       "36  moodlighting way fly ! best experience ever ! ...  \n",
       "37  freddieawards done done ! best airline around ...  \n",
       "38                            book flight hawaii ? ?   \n",
       "39  chat support working site : http : //t.co/vhp2...  \n",
       "40  view downtown los angeles , hollywood sign , b...  \n",
       "41  hey , first time flyer next week - excited ! '...  \n",
       "42  plz help win bid upgrade flight 2/27 lax -- - ...  \n",
       "43  unused ticket moved new city n't fly . fly exp...  \n",
       "44      flights leaving dallas seattle time feb 24 ?   \n",
       "45             'm elevategold good reason : rock ! !   \n",
       "46  dream http : //t.co/oa2drfaoq2 http : //t.co/l...  \n",
       "47                                     wow blew mind   \n",
       "48  ladygaga carrieunderwood last night tribute so...  \n",
       "49             ladygaga carrieunderwood entertaining   \n",
       "50  flight 769 's way ? supposed take 30 minutes a...  \n",
       "51  ladygaga carrieunderwood julie andrews way tho...  \n",
       "52                      wish flew atlanta ... soon ?   \n",
       "53  ladygaga carrieunderwood julie andrews . hands .   \n",
       "54         flights leaving dallas la february 24th ?   \n",
       "55  hi ! 'm excited 99 lga- gt ; dal deal- 've try...  \n",
       "56          know . need spotify stat guiltypleasures   \n",
       "57  ladygaga carrieunderwood 'm lady gaga ! ! ! am...  \n",
       "58               ladygaga carrieunderwood - carrie !   \n",
       "59  new marketing song ? https : //t.co/f2lfulcbq7...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','clean_tweets']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         neutral\n",
       "1        positive\n",
       "2         neutral\n",
       "3        negative\n",
       "4        negative\n",
       "           ...   \n",
       "14635    positive\n",
       "14636    negative\n",
       "14637     neutral\n",
       "14638    negative\n",
       "14639     neutral\n",
       "Name: airline_sentiment, Length: 14640, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'] = le.fit_transform(df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        2\n",
       "2        1\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "14635    2\n",
       "14636    0\n",
       "14637    1\n",
       "14638    0\n",
       "14639    1\n",
       "Name: airline_sentiment, Length: 14640, dtype: int32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = cv.fit_transform(df['clean_tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sparse_matrix, df['airline_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 14999), (3660, 14999), (10980,), (3660,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7609289617486339"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4795)\t1\n",
      "  (0, 11614)\t1\n",
      "  (1, 10406)\t1\n",
      "  (1, 14151)\t1\n",
      "  (1, 1964)\t1\n",
      "  (1, 4022)\t1\n",
      "  (1, 5654)\t1\n",
      "  (1, 12890)\t1\n",
      "  (2, 13290)\t1\n",
      "  (2, 9207)\t1\n",
      "  (2, 8810)\t1\n",
      "  (2, 9348)\t1\n",
      "  (2, 12906)\t1\n",
      "  (2, 2328)\t1\n",
      "  (2, 13469)\t1\n",
      "  (3, 10988)\t1\n",
      "  (3, 2052)\t1\n",
      "  (3, 3063)\t1\n",
      "  (3, 9698)\t1\n",
      "  (3, 5444)\t1\n",
      "  (3, 6719)\t1\n",
      "  (3, 5729)\t1\n",
      "  (3, 2261)\t1\n",
      "  (3, 8365)\t1\n",
      "  (3, 11046)\t1\n",
      "  :\t:\n",
      "  (14637, 10378)\t1\n",
      "  (14637, 2102)\t1\n",
      "  (14637, 3256)\t1\n",
      "  (14637, 2234)\t1\n",
      "  (14637, 3045)\t1\n",
      "  (14638, 5992)\t1\n",
      "  (14638, 3671)\t1\n",
      "  (14638, 2333)\t1\n",
      "  (14638, 9086)\t1\n",
      "  (14638, 8647)\t1\n",
      "  (14638, 10271)\t1\n",
      "  (14638, 4025)\t1\n",
      "  (14638, 12703)\t1\n",
      "  (14639, 9348)\t1\n",
      "  (14639, 5992)\t2\n",
      "  (14639, 11801)\t1\n",
      "  (14639, 8049)\t1\n",
      "  (14639, 9420)\t2\n",
      "  (14639, 10407)\t1\n",
      "  (14639, 14007)\t1\n",
      "  (14639, 10190)\t1\n",
      "  (14639, 8694)\t1\n",
      "  (14639, 12478)\t1\n",
      "  (14639, 10501)\t1\n",
      "  (14639, 10785)\t1\n"
     ]
    }
   ],
   "source": [
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
